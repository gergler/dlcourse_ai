{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ear8naev7HS"
      },
      "source": [
        "# Задание 4 - Перенос обучения (transfer learning) и тонкая настройка (fine-tuning)\n",
        "\n",
        "Одной из важнейшних техник в тренировке сетей - использовать заранее натренированные веса на более общей задачи в качестве начальной точки, а потом \"дотренировать\" их на конкретной.\n",
        "\n",
        "Такой подход и убыстряет обучение, и позволяет тренировать эффективные модели на маленьких наборах данных.\n",
        "\n",
        "В этом упражнении мы натренируем классификатор, который отличает хотдоги от не хотдогов!  \n",
        "(более подробно - https://www.youtube.com/watch?v=ACmydtFDTGs)\n",
        "\n",
        "Это задание требует доступа к GPU, поэтому его можно выполнять либо на компьютере с GPU от NVidia, либо в [Google Colab](https://colab.research.google.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FcXBeP1O7cnY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import csv\n",
        "import urllib\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "from socket import timeout\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# !pip3 install -q torch torchvision\n",
        "# !pip3 install -q Pillow==4.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHICz14hv7Hb"
      },
      "source": [
        "Сначала давайте скачаем данные с картинками. Это сделает код в следующей ячейке. Данные будут разделены на две части. На обучающей выборке, которая будет храниться в папке **train_kaggle**, мы будем строить наши модели, а на тестовой выборке **test_kaggle** будем предсказывать класс, к которому относится фотография (хотдог или нет).\n",
        "\n",
        "### Если вы в Google Colab!\n",
        "\n",
        "В нем можно запускать ноутбуки с доступом к GPU. Они не очень быстрые, зато бесплатные!\n",
        "Каждый ноутбук получает свой собственный environment c доступным диском итд.\n",
        "\n",
        "Через 90 минут отсуствия активности этот environment пропадает со всеми данными.\n",
        "Поэтому нам придется скачивать данные каждый раз."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ourBj07Arm3R",
        "outputId": "5984fc84-cd82-4b5a-abea-c4920b18f75f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-16 09:55:43--  https://storage.googleapis.com/dlcourse_ai/train.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.5.128, 74.125.133.128, 74.125.140.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.5.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 562348083 (536M) [application/zip]\n",
            "Saving to: ‘train.zip.1’\n",
            "\n",
            "train.zip.1         100%[===================>] 536.30M  40.3MB/s    in 13s     \n",
            "\n",
            "2022-04-16 09:55:57 (40.1 MB/s) - ‘train.zip.1’ saved [562348083/562348083]\n",
            "\n",
            "replace train_kaggle/pets_169.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace train_kaggle/frankfurter_6966.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace train_kaggle/pets_1141.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace train_kaggle/pets_1409.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace train_kaggle/pets_1578.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace train_kaggle/pets_1549.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace train_kaggle/pets_1549.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "Number of files in the train folder 4603\n",
            "--2022-04-16 09:57:00--  https://storage.googleapis.com/dlcourse_ai/test.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.5.128, 74.125.133.128, 74.125.140.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.5.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 140788786 (134M) [application/zip]\n",
            "Saving to: ‘test.zip.1’\n",
            "\n",
            "test.zip.1          100%[===================>] 134.27M  41.0MB/s    in 3.3s    \n",
            "\n",
            "2022-04-16 09:57:05 (41.0 MB/s) - ‘test.zip.1’ saved [140788786/140788786]\n",
            "\n",
            "replace test_kaggle/10000.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "Number of files in the test folder 1150\n"
          ]
        }
      ],
      "source": [
        "# Download train data\n",
        "!wget \"https://storage.googleapis.com/dlcourse_ai/train.zip\"\n",
        "!unzip -q \"train.zip\"\n",
        "\n",
        "train_folder = \"train_kaggle/\"\n",
        "# Count number of files in the train folder, should be 4603\n",
        "print('Number of files in the train folder', len(os.listdir(train_folder)))\n",
        "\n",
        "# Download test data\n",
        "!wget \"https://storage.googleapis.com/dlcourse_ai/test.zip\"\n",
        "!unzip -q \"test.zip\"\n",
        "\n",
        "test_folder = \"test_kaggle/\"\n",
        "# Count number of files in the test folder, should be 1150\n",
        "print('Number of files in the test folder', len(os.listdir(test_folder)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NNU-OD9O9ltP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device(\"cuda:0\") # Let's make sure GPU is available!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-paBge1dv7Hd"
      },
      "source": [
        "# Имплементируем свой Dataset для загрузки данных\n",
        "\n",
        "В этом задании мы реализуем свой собственный класс Dataset для загрузки данных. Его цель - загрузить данные с диска и выдать по ним тензор с входом сети, меткой и идентификатором картинки (так будет проще подготовить сабмит для kaggle на тестовых данных).\n",
        "\n",
        "Вот ссылка, где хорошо объясняется как это делать на примере: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "Ваш Dataset должен в качестве количества сэмплов выдать количество файлов в папке и уметь выдавать кортеж из сэмпла, метки по индексу и названия файла.\n",
        "Если название файла начинается со слов 'frankfurter', 'chili-dog' или 'hotdog' - метка положительная. Иначе отрицательная (ноль).\n",
        "\n",
        "И не забудьте поддержать возможность трансформации входа (аргумент `transforms`), она нам понадобится!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "bN2SPiJa9v5M",
        "outputId": "9e3a2a2d-4d1f-4646-d4f6-ae3415e5c9dd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-82a6548b422f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mvisualize_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-82a6548b422f>\u001b[0m in \u001b[0;36mvisualize_samples\u001b[0;34m(dataset, indices, title, count)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s %s/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-82a6548b422f>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_folder\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2852\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfromqimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     \u001b[0;34m\"\"\"Creates an image instance from a QImage image\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2854\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageQt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2856\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mImageQt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqt_is_installed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mpreinit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPpmImagePlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTiffImagePlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_binary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mi16be\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mi16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_binary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mi32be\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mi32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mImageFileDirectory_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMutableMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m     \"\"\"This class represents a TIFF tag directory.  To speed things up, we\n\u001b[1;32m    418\u001b[0m     \u001b[0mdon\u001b[0m\u001b[0;34m't decode tags unless they'\u001b[0m\u001b[0mre\u001b[0m \u001b[0masked\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py\u001b[0m in \u001b[0;36mImageFileDirectory_v2\u001b[0;34m()\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mTiffTags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLOAT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mTiffTags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOUBLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"double\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mTiffTags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIFD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"L\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"long\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m             ],\n\u001b[1;32m    689\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'PIL.TiffTags' has no attribute 'IFD'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2160x216 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class HotdogOrNotDataset(Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.transform = transform\n",
        "        self.main_folder = folder\n",
        "        self.folders = os.listdir(folder)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.folders)\n",
        "    \n",
        "    def __getitem__(self, index):        \n",
        "        img_path = os.path.join(self.main_folder , self.folders[index])\n",
        "        img = Image.open(img_path)\n",
        "        if (self.transform):\n",
        "          img = self.transform(img)\n",
        "        \n",
        "        if ('frankfurter' in img_path) or ('chili-dog' in img_path) or ('hotdog' in img_path):\n",
        "          y = 1 \n",
        "        else:\n",
        "          y = 0\n",
        "\n",
        "        img_id = index\n",
        "\n",
        "        return img, y, img_id\n",
        "\n",
        "def visualize_samples(dataset, indices, title=None, count=10):\n",
        "    # visualize random 10 samples\n",
        "    plt.figure(figsize=(count*3,3))\n",
        "    display_indices = indices[:count]\n",
        "    if title:\n",
        "        plt.suptitle(\"%s %s/%s\" % (title, len(display_indices), len(indices)))        \n",
        "    for i, index in enumerate(display_indices):    \n",
        "        x, y, _ = dataset[index]\n",
        "        plt.subplot(1,count,i+1)\n",
        "        plt.title(\"Label: %s\" % y)\n",
        "        plt.imshow(x)\n",
        "        plt.grid(False)\n",
        "        plt.axis('off')   \n",
        "\n",
        "orig_dataset = HotdogOrNotDataset(train_folder)\n",
        "indices = np.random.choice(np.arange(len(orig_dataset)), 7, replace=False)\n",
        "\n",
        "visualize_samples(orig_dataset, indices, \"Samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQNsUvYm4_2V"
      },
      "outputs": [],
      "source": [
        "# Let's make sure transforms work!\n",
        "dataset = HotdogOrNotDataset(train_folder, transform=transforms.RandomVerticalFlip(0.9))\n",
        "\n",
        "visualize_samples(dataset, indices, \"Samples with flip - a lot should be flipped!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_0T7czFv7Hg"
      },
      "source": [
        "# Создаем Dataset для тренировки\n",
        "\n",
        "И разделяем его на train и validation.\n",
        "На train будем обучать модель, на validation проверять ее качество, а соревнование Kaggle In-Class проведем на фотографиях из папки test_kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAvkoRx-9FsP"
      },
      "outputs": [],
      "source": [
        "# First, lets load the dataset\n",
        "train_dataset = HotdogOrNotDataset(train_folder, \n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize((224, 224)),\n",
        "                           transforms.ToTensor(),\n",
        "                           # Use mean and std for pretrained models\n",
        "                           # https://pytorch.org/docs/stable/torchvision/models.html\n",
        "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])                         \n",
        "                       ])\n",
        "                      )\n",
        "test_dataset = HotdogOrNotDataset(test_folder, \n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize((224, 224)),\n",
        "                           transforms.ToTensor(),\n",
        "                           # Use mean and std for pretrained models\n",
        "                           # https://pytorch.org/docs/stable/torchvision/models.html\n",
        "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])                         \n",
        "                       ])\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRnr8CPg7Hli"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "data_size = len(dataset)\n",
        "validation_fraction = .2\n",
        "\n",
        "\n",
        "val_split = int(np.floor((validation_fraction) * data_size))\n",
        "indices = list(range(data_size))\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "val_indices, train_indices = indices[:val_split], indices[val_split:]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
        "                                           sampler=train_sampler)\n",
        "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                         sampler=val_sampler)\n",
        "# Notice that we create test data loader in a different way. We don't have the labels.\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV4VdZ2gv7Hi"
      },
      "source": [
        "Наши обычные функции для тренировки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ek3KVQK7hJ6"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):    \n",
        "    loss_history = []\n",
        "    train_history = []\n",
        "    val_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Enter train mode\n",
        "        \n",
        "        loss_accum = 0\n",
        "        correct_samples = 0\n",
        "        total_samples = 0\n",
        "        for i_step, (x, y,_) in enumerate(train_loader):\n",
        "          \n",
        "            x_gpu = x.to(device)\n",
        "            y_gpu = y.to(device)\n",
        "            prediction = model(x_gpu)    \n",
        "            loss_value = loss(prediction, y_gpu)\n",
        "            optimizer.zero_grad()\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            _, indices = torch.max(prediction, 1)\n",
        "            correct_samples += torch.sum(indices == y_gpu)\n",
        "            total_samples += y.shape[0]\n",
        "            \n",
        "            loss_accum += loss_value\n",
        "\n",
        "        ave_loss = loss_accum / i_step\n",
        "        train_accuracy = float(correct_samples) / total_samples\n",
        "        val_accuracy = compute_accuracy(model, val_loader)\n",
        "        \n",
        "        loss_history.append(float(ave_loss))\n",
        "        train_history.append(train_accuracy)\n",
        "        val_history.append(val_accuracy)\n",
        "        \n",
        "        print(\"Average loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss, train_accuracy, val_accuracy))\n",
        "        \n",
        "    return loss_history, train_history, val_history\n",
        "        \n",
        "def compute_accuracy(model, loader):\n",
        "    model.eval() # Evaluation mode \n",
        "    corrects = 0 \n",
        "    total = 0 \n",
        "    for i_step, (x, y,_) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        prediction = torch.argmax(model(x) , 1)    \n",
        "        for i in range(len(prediction)):\n",
        "            if prediction[i] == y[i]:\n",
        "                corrects += float(1)\n",
        "        total += float(len(prediction))            \n",
        "\n",
        "    return float(corrects / total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XYxxkoTv7Hi"
      },
      "source": [
        "# Использование заранее натренированной сети (pretrained network)\n",
        "\n",
        "Чаще всего в качестве заранее натренированной сети используется сеть, натренированная на данных ImageNet с 1M изображений и 1000 классами.\n",
        "\n",
        "PyTorch включает такие натренированные сети для различных архитектур (https://pytorch.org/docs/stable/torchvision/models.html)  \n",
        "Мы будем использовать ResNet18.\n",
        "\n",
        "Для начала посмотрим, что выдает уже натренированная сеть на наших картинках. То есть, посмотрим к какому из 1000 классов их отнесет сеть.\n",
        "\n",
        "Запустите модель на 10 случайных картинках из датасета и выведите их вместе с классами с наибольшей вероятностью.  \n",
        "В коде уже есть код, который формирует соответствие между индексами в выходном векторе и классами ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnvXSmtyLAgz"
      },
      "outputs": [],
      "source": [
        "# Thanks to https://discuss.pytorch.org/t/imagenet-classes/4923/2\n",
        "def load_imagenet_classes():\n",
        "    classes_json = urllib.request.urlopen('https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json').read()\n",
        "    classes = json.loads(classes_json)\n",
        "    \n",
        "    # TODO: Process it to return dict of class index to name\n",
        "    return { int(k): v[-1] for k, v in classes.items()}\n",
        "    \n",
        "model = models.resnet18(pretrained=True)\n",
        "model = model.to(device)\n",
        "def visualize(model ,dataset, indices, title=None, count=10):\n",
        "\n",
        "    classes= load_imagenet_classes()\n",
        "\n",
        "    plt.figure(figsize=(count*3,3))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for x,y,_ in dataset:\n",
        "            x = x.to(device)\n",
        "            prediction = torch.argmax(model(x) , 1) \n",
        "            prediction = np.array(prediction.cpu())   \n",
        "            \n",
        "            for i in range(count):\n",
        "                plt.subplot(1,count,i+1)\n",
        "                plt.title(\"Label: %s\" % classes[prediction[i]])\n",
        "                img = x.cpu().data[i]  \n",
        "                img = img.permute(1, 2, 0) * np.array([0.5, 0.5, 0.5]) +np.array([0.3, 0.3, 0.3]) \n",
        "                img = np.clip(img,0,1)\n",
        "                plt.imshow(img)\n",
        "                plt.grid(False)\n",
        "                plt.axis('off')\n",
        "            return\n",
        "\n",
        "indices = np.random.choice(np.arange(len(dataset)), 10, replace=False)\n",
        "\n",
        "visualize(model , train_loader, indices, None, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a-3a1ZFGEw_"
      },
      "source": [
        "# Перенос обучения (transfer learning) - тренировать только последний слой\n",
        "\n",
        "Существует несколько вариантов переноса обучения, мы попробуем основные.  \n",
        "Первый вариант - заменить последний слой на новый и тренировать только его, заморозив остальные."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCWMUWmr7t5g"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "# TODO: Freeze all the layers of this model and add a new output layer\n",
        "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "nfeatures = model.fc.in_features\n",
        "model.fc = nn.Linear(nfeatures, 2)\n",
        "\n",
        "parameters = model.fc.parameters()\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD( parameters, lr=0.001, momentum=0.9)\n",
        "loss_history, train_history, val_history = train_model(model, train_loader, val_loader, loss, optimizer, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dDH4WfaB2Il"
      },
      "source": [
        "# Перенос обучения (transfer learning) - тренировать всю модель\n",
        "\n",
        "Второй вариант - точно так же заменить последгний слой на новый и обучать всю модель целиком."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ss0jilyvuOh"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "# TODO: Add a new output layer and train the whole model\n",
        "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "nfeatures = model.fc.in_features\n",
        "model.fc = nn.Linear(nfeatures, 2)\n",
        "\n",
        "parameters = model.parameters()  \n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD( parameters, lr=0.001, momentum=0.9)\n",
        "\n",
        "loss_history, train_history, val_history = train_model(model, train_loader, val_loader, loss, optimizer, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meQt_vDCs9cc"
      },
      "source": [
        "# Перенос обучения (transfer learning) - разные скорости обучения для разных слоев\n",
        "\n",
        "И наконец последний вариант, который мы рассмотрим - использовать разные скорости обучения для новых и старых слоев"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evro9ksXGs9u"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "# TODO: Add a new output layer\n",
        "# Train new layer with learning speed 0.001 and old layers with 0.0001\n",
        "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "nfeatures = model.fc.in_features\n",
        "model.fc = nn.Linear(nfeatures, 2)\n",
        "\n",
        "first_layers = list(model.parameters())[:-2]\n",
        "second_layers = model.fc.parameters()\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD([{'params': first_layers},\n",
        "           {'params': second_layers, 'lr': 0.001}], lr=0.0001, momentum=0.9)\n",
        "\n",
        "model = model.to(device)\n",
        "loss_history, train_history, val_history = train_model(model, train_loader, val_loader, loss, optimizer, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbWF8eWrv7Hl"
      },
      "source": [
        "# Визуализируем метрики и ошибки модели\n",
        "\n",
        "Попробуем посмотреть, где модель ошибается - визуализируем ложные срабатывания (false positives) и ложноотрицательные срабатывания (false negatives).\n",
        "\n",
        "Для этого мы прогоним модель через все примеры и сравним ее с истинными метками (ground truth)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieEzZUglJAUB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "class SubsetSampler(Sampler):\n",
        "    r\"\"\"Samples elements with given indices sequentially\n",
        "\n",
        "    Arguments:\n",
        "        data_source (Dataset): dataset to sample from\n",
        "        indices (ndarray): indices of the samples to take\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, indices):\n",
        "        self.indices = indices\n",
        "\n",
        "    def __iter__(self):\n",
        "        return (self.indices[i] for i in range(len(self.indices)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    \n",
        "    \n",
        "def evaluate_model(model, dataset, indices):\n",
        "    \"\"\"\n",
        "    Computes predictions and ground truth labels for the indices of the dataset\n",
        "    \n",
        "    Returns: \n",
        "    predictions: np array of booleans of model predictions\n",
        "    grount_truth: np array of boolean of actual labels of the dataset\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.cpu()\n",
        "    with torch.no_grad():\n",
        "        val_loader = torch.utils.data.DataLoader(dataset, batch_size=len(indices),sampler=SubsetSampler(indices))\n",
        "        for x, y, _ in val_loader:\n",
        "            prediction = torch.argmax(model(x), dim=1)\n",
        "            gt = y\n",
        "                                                  \n",
        "    return prediction.numpy(), gt.numpy()\n",
        "\n",
        "predictions, gt = evaluate_model(model, train_dataset, val_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0bcioK6JBDK"
      },
      "source": [
        "И теперь можно визуализировать false positives и false negatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMmaPfdeKk9H"
      },
      "outputs": [],
      "source": [
        "false_positive_indices = [val_indices[i] for i in np.where( predictions & ~gt)[0]]\n",
        "visualize_samples(orig_dataset, false_positive_indices, \"False positives\")\n",
        "\n",
        "false_negative_indices = [val_indices[i] for i in np.where( ~predictions & gt)[0]]\n",
        "visualize_samples(orig_dataset, false_negative_indices, \"False negatives\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoDeVjN4HZSV"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as metrics\n",
        "def binary_classification_metrics(prediction, ground_truth):\n",
        "    fp = np.sum((prediction != ground_truth) & (prediction == 1) & (ground_truth == 0))\n",
        "    fn = np.sum((prediction != ground_truth) & (prediction == 0) & (ground_truth == 1))\n",
        "    tp = np.sum((prediction == ground_truth) & (prediction == 1))\n",
        "    tn = np.sum((prediction == ground_truth) & (prediction == 1))\n",
        "    \n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    \n",
        "    return precision, recall, f1\n",
        "\n",
        "precision, recall, f1 = binary_classification_metrics(predictions, gt)\n",
        "print(\"F1: %4.3f, P: %4.3f, R: %4.3f\" % (f1, precision, recall))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_O9qiYySvuj"
      },
      "source": [
        "# Что будет в конце вы уже поняли\n",
        "\n",
        "Натренируйте лучшую модель на основе `resnet18`, меняя только процесс тренировки.\n",
        "Выбирайте лучшую модель по F1 score.\n",
        "\n",
        "Как всегда, не забываем:\n",
        "- побольше агментаций!\n",
        "- перебор гиперпараметров\n",
        "- различные оптимизаторы\n",
        "- какие слои тюнить\n",
        "- learning rate annealing\n",
        "- на какой эпохе останавливаться\n",
        "\n",
        "Наша цель - довести F1 score на validation set до значения, большего **0.93**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6mhfdQ9K-N3"
      },
      "outputs": [],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "nfeatures = model.fc.in_features\n",
        "model.fc = nn.Linear(nfeatures, 2)\n",
        "\n",
        "first_layers = list(model.parameters())[:-2]\n",
        "second_layers = model.fc.parameters()\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD([{'params': first_layers},\n",
        "           {'params': second_layers, 'lr': 0.0005}], lr=0.0002, momentum=0.99)\n",
        "\n",
        "model = model.to(device)\n",
        "loss_history, train_history, val_history = train_model(model, train_loader, val_loader, loss, optimizer, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6xExdw8JB1l"
      },
      "outputs": [],
      "source": [
        "# Let's check how it performs on validation set!\n",
        "predictions, ground_truth = evaluate_model(model, dataset, val_indices)\n",
        "precision, recall, f1 = binary_classification_metrics(predictions, ground_truth)\n",
        "print(\"F1: %4.3f, P: %4.3f, R: %4.3f\" % (precision, recall, f1))\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.plot(train_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx6jeLy7v7Hn"
      },
      "source": [
        "## Визуализируйте ошибки лучшей модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFUeNOm1VACr"
      },
      "outputs": [],
      "source": [
        "false_positive_indices = [val_indices[i] for i in np.where(predictions & ~gt)[0]]\n",
        "visualize_samples(orig_dataset, false_positive_indices, \"False positives\")\n",
        "\n",
        "false_negative_indices = [val_indices[i] for i in np.where(~predictions & gt)[0]]\n",
        "visualize_samples(orig_dataset, false_negative_indices, \"False negatives\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm6Cbgsov7Hn"
      },
      "source": [
        "# Необязательное задание с большой звездочкой\n",
        "\n",
        "Поучавствуйте в Kaggle In-Class Hot Dog Recognition Challenge!  \n",
        "Это соревнование сделано специально для курса и в нем учавствуют только те, кто проходит курс.\n",
        "\n",
        "В нем участники соревнуются в качестве натренированных моделей, загружая на сайт предсказания своих моделей на тестовой выборке. Разметка тестовой выборке участникам недоступна.\n",
        "Более подробно о правилах соревнования ниже.\n",
        "\n",
        "Те, кто проходят курс лично, за высокое место в соревновании получат дополнительные баллы.\n",
        "\n",
        "Здесь уже можно использовать и другие базовые архитектуры кроме `resnet18`, и ансамбли, и другие трюки тренировки моделей.\n",
        "\n",
        "Вот ссылка на соревнование:\n",
        "https://www.kaggle.com/c/hotdogornot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl5g1Lupv7Ho"
      },
      "outputs": [],
      "source": [
        "image_id = []\n",
        "predictions = []\n",
        "model.eval()\n",
        "for x,_,id_img in test_loader:\n",
        "    # TODO : Напишите код для предсказания меток (1 = есть хотдог, 0 = хотдога нет)\n",
        "    # Код должен возвратить список из id картинки и метку predictions\n",
        "    # image id - это название файла картинки, например '10000.jpg'\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viTuM4Eev7Ho"
      },
      "outputs": [],
      "source": [
        "# Так можно создать csv файл, чтобы затем загрузить его на kaggle\n",
        "# Ожидаемый формат csv-файла:\n",
        "# image_id,label\n",
        "# 10000.jpg,1\n",
        "# 10001.jpg,1\n",
        "# 10002.jpg,0\n",
        "# 10003.jpg,1\n",
        "# 10004.jpg,0\n",
        "\n",
        "with open('subm.csv', 'w') as submissionFile:\n",
        "    writer = csv.writer(submissionFile)\n",
        "    writer.writerow(['image_id', 'label'])\n",
        "    writer.writerows(zip(image_id,predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spvytGXfv7Ho"
      },
      "outputs": [],
      "source": [
        "# А так можно скачать файл с Google Colab\n",
        "files.download('subm.csv') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNSPdyQUv7Ho"
      },
      "source": [
        "### Небольшое введение в Kaggle для тех, кто не слышал об этой платформе раньше"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Acv9yiuv7Hp"
      },
      "source": [
        "В основе своей Kaggle - это платформа для проведения соревнований по машинному обучению. Появилась она в 2010 и, пожалуй, стала самой популярной и известной из всех существующих площадок по машинному обучению. Надо сказать, что Kaggle - это не только соревнования, но и сообщество людей, увлеченных машинным обучением. А судя по Википедии, в 2017 году отметка зарегистрированных пользователей перевалила за миллион. Есть там и обучающие материалы, возможность задавать вопросы, делиться кодом и идеями - просто мечта. \n",
        "\n",
        "### Как проходят соревнования? \n",
        "Обычно участники скачивают данные для обучения моделей (train data), чтобы затем делать предсказания на тестовых данных (test data). Обучающая выборка содержит как сами данные, так и правильные метки (значения зависимой переменной), чтобы можно было обучить модель. Но тестовые данные ответа не содержат - и нашей целью является предсказание меток по имеющимся данным. Файл с ответами для каждого наблюдения из тестовой выборки загружается на Kaggle и оценивается в соответствии с выбранной метрикой соревнования, а результат является публичным и показывается в общей таблице (ее называют еще лидербордом - leaderboard) - чтобы появилось желание посоревноваться и создать еще более сильную модель. В \"настоящих\" соревнованиях, которые проходят на Kaggle, есть и денежное вознаграждение для тех участников, кто занимает первые места на лидерборде. Например, в [этом](https://www.kaggle.com/c/zillow-prize-1#description) соревновании, человек, занявший первое место, получил около 1 000 000 долларов. \n",
        "\n",
        "Тестовые данные делятся случайным образом в некоторой пропорции. И пока соревнование идет, на лидерборде показываются очки и рейтинг участников только по одной части (Public Leaderboard). А вот когда соревнование заканчивается, то рейтинг участников составляется по второй части тестовых данных (Private Leaderboard). И часто можно видеть, как люди занимавшие первые места на публичной части тестовых данных, оказываются далеко не первыми на закрытой части тестовых данных. Зачем это сделано? Есть несколько причин, но, пожалуй, самой фундаментальной является идея недообучения-переобучения. Всегда возможно, что наша модель настроилась на конкретную выборку, но как она поведет себя на тех данных, которые еще не видела? Разбиение тестовых данных на публичную и скрытую части сделано для того, чтобы отобрать модели, которые имеют большую обобщающую способность. Одним из лозунгов участников соревнований стал \"Доверяйте своей локальной кросс-валидации\" (Trust your CV!). Есть очень большой соблазн оценивать свою модель по публичной части лидерборда, но лучшей стратегией будет выбирать ту модель, которая дает лучшую метрику на кросс-валидации на обучающей выборке. \n",
        "\n",
        "В нашем соревновании публичная часть лидерборда составляет 30%, а скрытая 70%. Вы можете делать до двух попыток в день, а оцениваться попытки будут по F1-мере. Удачи и доверяйте своей локальной валидации! В конце соревнования у вас будет возможность выбрать 2 из всех совершенных попыток - лучшая из этих двух и будет засчитана вам на скрытой части тестовых данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxN0ha8gv7Hp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HotdogOrNot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}